'elu' :正の値を抑える。負の値も抑える -X to + N 

'hardSigmoid':

'linear':入れたのそのまま

'relu':+のみそのまま 0 to N
'relu6':+は最大6まで 0 to Max(N,6)
'selu':-は-1以上にもスケールする
'sigmoid':値を0～1に滑らかにする 0 to +1
'softmax':全体を1にまとめる
'softplus':-の入力もある程度+にした上で+に加算する
'softsign':ステップ関数をなだらかに-1 to +1
'tanh':かなり滑らかに-1 to +1
